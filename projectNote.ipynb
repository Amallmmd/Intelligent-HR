{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amallmuhammed/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key= openai_api_key,temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The biggest country in the world by land area is Russia, with a total area of 17,098,242 square kilometers. However, if we consider total area (including land and water), then Canada is the largest country with a total area of 9,984,670 square kilometers.\n"
     ]
    }
   ],
   "source": [
    "s = 'Which is the biggest country in the world'\n",
    "print(llm.invoke(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_template =  PromptTemplate(input_variables=['Job_Role'], \n",
    "                               template='What are the basic qualification required for {Job_Role}')\n",
    "job_chain = LLMChain(llm=llm, prompt=job_template, output_key='Skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_template = PromptTemplate(input_variables=['Skills'],\n",
    "                                template='The skills required to become {Job_Role} are: {Skills}')\n",
    "skill_chain = LLMChain(llm=llm ,prompt=skill_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain = SequentialChain(chains=[job_chain,skill_chain],\n",
    "                        input_variables=['Job_Role'],\n",
    "                        output_variables=['Skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chain.invoke() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJob_Role\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData Scientist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Chain.invoke() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "chain.invoke(Job_Role='Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA Chat using Open AI llm\n",
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import re\n",
    "\n",
    "# # Set up your OpenAI API key\n",
    "# openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# def process_job_description(text):\n",
    "#     \"\"\"Simplifies the job description for easier question generation.\"\"\"\n",
    "\n",
    "#     # Basic processing (replace with more sophisticated techniques if needed)\n",
    "#     processed_text = text.lower()  # Makes text lowercase\n",
    "#     processed_text = re.sub(r\"[^\\w\\s]\", \"\", processed_text)  # Removes punctuation (adjust as needed) \n",
    "#     return processed_text\n",
    "\n",
    "# def generate_questions(processed_description):\n",
    "#     \"\"\"Uses OpenAI's text generation to create questions.\"\"\"\n",
    "\n",
    "#     response = openai.Completion.create(\n",
    "#         engine=\"text-davinci-003\",  # Adjust the engine if needed\n",
    "#         prompt=f\"Generate a few insightful questions about the following job description: {processed_description}\",\n",
    "#         max_tokens=150,  # Controls the length of the response\n",
    "#         n=1,\n",
    "#         stop=None,\n",
    "#         temperature=0.7,  # Encourages a bit of creativity\n",
    "#     )\n",
    "#     return response.choices[0].text.strip().split(\"\\n\")\n",
    "\n",
    "# def ask_questions(questions):\n",
    "#     \"\"\"Asks the generated questions and gets user answers.\"\"\"\n",
    "\n",
    "#     for question in questions:\n",
    "#         answer = input(question + \"\\nAnswer: \")\n",
    "#         print(f\"You answered: {answer}\\n\")  \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     job_description = input(\"Enter a job description: \\n\")\n",
    "#     processed_description = process_job_description(job_description)\n",
    "#     questions = generate_questions(processed_description)\n",
    "#     ask_questions(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading \n        **Job Title:** Software Engineer\n\n        **Responsibilities:**\n        * Design, develop, and test software applications.\n        * Collaborate with cross-functional teams.\n        * Maintain and improve existing codebases.\n\n        **Qualifications:**\n        * Bachelor's degree in Computer Science or related field.\n        * Proficiency in Python or Java.\n        * Experience with web development frameworks.\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_community/document_loaders/text.py:41\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 63] File name too long: \"\\n        **Job Title:** Software Engineer\\n\\n        **Responsibilities:**\\n        * Design, develop, and test software applications.\\n        * Collaborate with cross-functional teams.\\n        * Maintain and improve existing codebases.\\n\\n        **Qualifications:**\\n        * Bachelor's degree in Computer Science or related field.\\n        * Proficiency in Python or Java.\\n        * Experience with web development frameworks.\\n        \"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     36\u001b[0m job_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m        **Job Title:** Software Engineer\u001b[39m\n\u001b[1;32m     38\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m        * Experience with web development frameworks.\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 50\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_qa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Start a conversational session\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mcreate_qa_chain\u001b[0;34m(job_description)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the job description\u001b[39;00m\n\u001b[1;32m     16\u001b[0m text_loader \u001b[38;5;241m=\u001b[39m TextLoader(job_description)\n\u001b[0;32m---> 17\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mtext_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Split the text into smaller chunks\u001b[39;00m\n\u001b[1;32m     20\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_community/document_loaders/text.py:57\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     59\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path}\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading \n        **Job Title:** Software Engineer\n\n        **Responsibilities:**\n        * Design, develop, and test software applications.\n        * Collaborate with cross-functional teams.\n        * Maintain and improve existing codebases.\n\n        **Qualifications:**\n        * Bachelor's degree in Computer Science or related field.\n        * Proficiency in Python or Java.\n        * Experience with web development frameworks.\n        "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import langchain as lc\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_qa_chain(job_description):\n",
    "    \"\"\"Creates the QA chain using the provided job description.\"\"\"\n",
    "\n",
    "    # Initialize the LLM\n",
    "    llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key) \n",
    "\n",
    "    # Load the job description\n",
    "    text_loader = TextLoader(job_description)\n",
    "    documents = text_loader.load()\n",
    "\n",
    "    # Split the text into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Create a FAISS vectorstore to store document embeddings\n",
    "    vectorstore = FAISS.from_documents(docs, llm)\n",
    "\n",
    "    # The QA chain for generating questions and answers \n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, \n",
    "        chain_type=\"conversational\",\n",
    "        retriever=vectorstore.as_retriever()\n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "# Example usage\n",
    "job_description = \"\"\"\n",
    "        **Job Title:** Software Engineer\n",
    "\n",
    "        **Responsibilities:**\n",
    "        * Design, develop, and test software applications.\n",
    "        * Collaborate with cross-functional teams.\n",
    "        * Maintain and improve existing codebases.\n",
    "\n",
    "        **Qualifications:**\n",
    "        * Bachelor's degree in Computer Science or related field.\n",
    "        * Proficiency in Python or Java.\n",
    "        * Experience with web development frameworks.\n",
    "        \"\"\"\n",
    "\n",
    "qa_chain = create_qa_chain(job_description)\n",
    "\n",
    "# Start a conversational session\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    response = qa_chain.run(question)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amallmuhammed/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task question-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     17\u001b[0m job_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe are looking for a software engineer with experience in Python and machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(question)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[0;34m(job_description)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_questions\u001b[39m(job_description):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Initialize the QA pipeline\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     qa_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalhalla/t5-base-qg-hl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Prepare the prompt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a job description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mjob_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCan you provide more details about the job?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/transformers/pipelines/__init__.py:859\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    853\u001b[0m             class_ref,\n\u001b[1;32m    854\u001b[0m             model,\n\u001b[1;32m    855\u001b[0m             code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[1;32m    856\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    857\u001b[0m         )\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 859\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/transformers/pipelines/__init__.py:543\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Dict, Any]:\n\u001b[1;32m    499\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/transformers/pipelines/base.py:1271\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1271\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task question-generation, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_questions(job_description):\n",
    "    # Initialize the QA pipeline\n",
    "    qa_pipeline = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"This is a job description:\\n{job_description}\\n\\nCan you provide more details about the job?\"\n",
    "\n",
    "    # Generate questions\n",
    "    questions = qa_pipeline(prompt, max_length=50, do_sample=True, top_p=0.95, top_k=60)\n",
    "\n",
    "    # Extract and return the generated questions\n",
    "    return [question['generated_text'] for question in questions]\n",
    "\n",
    "# Example usage\n",
    "job_description = \"We are looking for a software engineer with experience in Python and machine learning.\"\n",
    "questions = generate_questions(job_description)\n",
    "for question in questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nretriever\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create the conversational chain\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Start the conversation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWelcome to the Job Description QA! You can ask questions about the job description, and I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll try my best to answer them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:387\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    381\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    382\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    383\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    384\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    385\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    386\u001b[0m )\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondense_question_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nretriever\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "loader = TextLoader('jobdesc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Create a vector store index\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Initialize the OpenAI LLM\n",
    "llm = OpenAI(temperature=0.5, openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the conversational chain\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, index.vectorstore)\n",
    "\n",
    "# Start the conversation\n",
    "print(\"Welcome to the Job Description QA! You can ask questions about the job description, and I'll try my best to answer them.\")\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    chat_history.append((query, result[\"answer\"]))\n",
    "    print(f\"Assistant: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
