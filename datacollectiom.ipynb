{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.retrievers import MergerRetriever, ContextualCompressionRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "print(\"Embeddings loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Presets using Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='VISHNU PRAKASH\\nData Scientist\\nvishnucheppanam@gmail.com — +91 80 780 43398\\nlinkedin.com/in/vishnuprksh — github.com/vishnuprksh — vishnuprakash.online\\nPROFILE\\nData Science aspirant with hands-on experience in machine learning and deep learning, seeking a\\nchallenging role as a Data Scientist/Machine Learning Engineer. Eager to leverage skills in Artificial\\nIntelligence, Neural Networks, and Algorithms to contribute to the companies and personal growth.\\nPROJECTS\\nGemInsights Git Link , Automating Exploratory Data Analysis (EDA) and Insight generation with\\nthe help of latest Gemini engine by Google\\n•EDA generation with the help of AutoViz and Image analysis with Gemini-Pro-Vision.\\n•Hallucination check with Trulense-Eval.\\n•UI with Streamlit.\\nTradePilot Git Link , Empowering stock market price prediction with sentiment analysis from News,\\nTwitter and Reddit\\n•Stock price history collected from Yahoo Finance and Text data scraped from Economic Times,\\nReddit, and Twitter.\\n•Sentiment analysis with Finbert (Hugging Face) and Summary with Google Gemini-Pro.\\n•UI with FastAPI.\\nSKILLS\\nMachine Learning ( scikit ,NLTK ,spaCy ,Numpy ,Pandas ,NLP )\\nDeep Learning ( Keras ,PyTorch ,TensorFlow ,RNN ,LSTM ,CNN )\\nData Visualization ( Tableau ,PowerBI )\\nDatabase Management ( MySQL ,MongoDB )\\nProgramming Languages ( Python ,JavaScript ,C,C++)\\nUI Development ( FastAPI ,HTML ,CSS,Streamlit ,Django ).\\nEDUCATION\\nData Science Self Learning Bootcamp, Brototype Ernakulam, India\\n•Covered topics including Data Science, Machine Learning, Deep Learning, Generative AI,\\nNLP, Feature Engineering, RAG Modeling, Data Structures, Statistics and Probability, Web\\nDevelopment.\\n•Trained in communication and personality development.\\nB.Tech in Naval Architecture, CUSAT Kerala, India\\n•Studied Ship Building, Stability, Hydrodynamics, Mechanics, Graphics, and Computer Science.\\nPROFESSIONAL EXPERIENCE\\nNaval Architect, SEDS 07/2020 – 03/2023 — Kochi, India\\n•Contributed to the development of shipbuilding projects, adhering to industry standards.\\nProduction Supervisor, Navgathi 07/2019 – 03/2020 — Kochi, India\\n•Managed the production team for boat production.\\nCERTIFICATES\\nMachine Learning (Kaggle) ,Data Science (Coursera) ,HTML (Codeacademy) .', metadata={'source': 'Testing/resume/VishnuPrakash_Resume.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "# Loading Resume\n",
    "resume = PyPDFLoader(\"Testing/resume/VishnuPrakash_Resume.pdf\")\n",
    "resume_load = resume.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "resume_splitted = text_splitter.split_documents(resume_load)\n",
    "\n",
    "print(resume_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Junior Machine Learning Engineer at our cutting-edge company, your role is crucial in developing the future of AI chatbots and Large Language Models (LLMs) like Llama2, with a special emphasis on integrating these innovations with WIX platforms. Your work will involve a deep dive into AWS services, Data ETL, GitHub for code collaboration, and the hands-on building of models, all while leveraging your strong quantitative and programming background. This role demands not only technical expertise but also a creative approach to problem-solving and code development. You will work in close partnership with the CTO, ensuring that your contributions leave a lasting impact on our AI-driven solutions. Additionally, your collaboration with front-end developers is key to weaving AI functionalities into WIX-based applications seamlessly, enhancing user experiences and pushing the boundaries of what our digital platforms can achieve.\n"
     ]
    }
   ],
   "source": [
    "# loading job description\n",
    "path = \"Testing/resume/job_desc.txt\"\n",
    "with open(path, \"r\") as file:\n",
    "    job_desc = file.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "jd_splitted = text_splitter.split_text(job_desc)\n",
    "\n",
    "print(jd_splitted[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorDB for resume\n",
    "chroma_resume = Chroma.from_documents(\n",
    "        resume_splitted,embeddings,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},persist_directory=\"vector_storage/resume_store\" # l2 is the default\n",
    "    )\n",
    "# vectorDB for JD\n",
    "chroma_jd = Chroma.from_texts(\n",
    "    jd_splitted,embeddings,collection_metadata = {\"hnsw:space\": \"cosine\"},\n",
    "    persist_directory=\"vector_storage/jd_store\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x14b6ce410>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vector stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_chroma_resume = Chroma(persist_directory=\"vector_storage/resume_store\",embedding_function=embeddings)\n",
    "load_chroma_jd = Chroma(persist_directory=\"vector_storage/jd_store\",embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Merge Retriever and Perform semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x14b7a9710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chroma_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_resume = load_chroma_resume.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":1})\n",
    "\n",
    "retriever_jd = load_chroma_jd.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x14b7a9710> search_kwargs={'k': 1}\n"
     ]
    }
   ],
   "source": [
    "print(retriever_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = MergerRetriever(retrievers=[retriever_resume, retriever_jd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISHNU PRAKASH\n",
      "Data Scientist\n",
      "vishnucheppanam@gmail.com — +91 80 780 43398\n",
      "linkedin.com/in/vishnuprksh — github.com/vishnuprksh — vishnuprakash.online\n",
      "PROFILE\n",
      "Data Science aspirant with hands-on experience in machine learning and deep learning, seeking a\n",
      "challenging role as a Data Scientist/Machine Learning Engineer. Eager to leverage skills in Artificial\n",
      "Intelligence, Neural Networks, and Algorithms to contribute to the companies and personal growth.\n",
      "PROJECTS\n",
      "GemInsights Git Link , Automating Exploratory Data Analysis (EDA) and Insight generation with\n",
      "the help of latest Gemini engine by Google\n",
      "•EDA generation with the help of AutoViz and Image analysis with Gemini-Pro-Vision.\n",
      "•Hallucination check with Trulense-Eval.\n",
      "•UI with Streamlit.\n",
      "TradePilot Git Link , Empowering stock market price prediction with sentiment analysis from News,\n",
      "Twitter and Reddit\n",
      "•Stock price history collected from Yahoo Finance and Text data scraped from Economic Times,\n",
      "Reddit, and Twitter.\n",
      "•Sentiment analysis with Finbert (Hugging Face) and Summary with Google Gemini-Pro.\n",
      "•UI with FastAPI.\n",
      "SKILLS\n",
      "Machine Learning ( scikit ,NLTK ,spaCy ,Numpy ,Pandas ,NLP )\n",
      "Deep Learning ( Keras ,PyTorch ,TensorFlow ,RNN ,LSTM ,CNN )\n",
      "Data Visualization ( Tableau ,PowerBI )\n",
      "Database Management ( MySQL ,MongoDB )\n",
      "Programming Languages ( Python ,JavaScript ,C,C++)\n",
      "UI Development ( FastAPI ,HTML ,CSS,Streamlit ,Django ).\n",
      "EDUCATION\n",
      "Data Science Self Learning Bootcamp, Brototype Ernakulam, India\n",
      "•Covered topics including Data Science, Machine Learning, Deep Learning, Generative AI,\n",
      "NLP, Feature Engineering, RAG Modeling, Data Structures, Statistics and Probability, Web\n",
      "Development.\n",
      "•Trained in communication and personality development.\n",
      "B.Tech in Naval Architecture, CUSAT Kerala, India\n",
      "•Studied Ship Building, Stability, Hydrodynamics, Mechanics, Graphics, and Computer Science.\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Naval Architect, SEDS 07/2020 – 03/2023 — Kochi, India\n",
      "•Contributed to the development of shipbuilding projects, adhering to industry standards.\n",
      "Production Supervisor, Navgathi 07/2019 – 03/2020 — Kochi, India\n",
      "•Managed the production team for boat production.\n",
      "CERTIFICATES\n",
      "Machine Learning (Kaggle) ,Data Science (Coursera) ,HTML (Codeacademy) .\n",
      "As a Junior Machine Learning Engineer at our cutting-edge company, your role is crucial in developing the future of AI chatbots and Large Language Models (LLMs) like Llama2, with a special emphasis on integrating these innovations with WIX platforms. Your work will involve a deep dive into AWS services, Data ETL, GitHub for code collaboration, and the hands-on building of models, all while leveraging your strong quantitative and programming background. This role demands not only technical expertise but also a creative approach to problem-solving and code development. You will work in close partnership with the CTO, ensuring that your contributions leave a lasting impact on our AI-driven solutions. Additionally, your collaboration with front-end developers is key to weaving AI functionalities into WIX-based applications seamlessly, enhancing user experiences and pushing the boundaries of what our digital platforms can achieve.\n"
     ]
    }
   ],
   "source": [
    "for chunks in merged.get_relevant_documents(\"Machine Learning\"):\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_resumes = chroma_resume.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":1})\n",
    "\n",
    "retriever_jds = chroma_jd.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x14b1aac10> search_kwargs={'k': 1}\n"
     ]
    }
   ],
   "source": [
    "print(retriever_jds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeds = MergerRetriever(retrievers=[retriever_resumes, retriever_jds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrievers=[VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x14b6ce410>, search_kwargs={'k': 1}), VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x14b1aac10>, search_kwargs={'k': 1})]\n"
     ]
    }
   ],
   "source": [
    "print(mergeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISHNU PRAKASH\n",
      "Data Scientist\n",
      "vishnucheppanam@gmail.com — +91 80 780 43398\n",
      "linkedin.com/in/vishnuprksh — github.com/vishnuprksh — vishnuprakash.online\n",
      "PROFILE\n",
      "Data Science aspirant with hands-on experience in machine learning and deep learning, seeking a\n",
      "challenging role as a Data Scientist/Machine Learning Engineer. Eager to leverage skills in Artificial\n",
      "Intelligence, Neural Networks, and Algorithms to contribute to the companies and personal growth.\n",
      "PROJECTS\n",
      "GemInsights Git Link , Automating Exploratory Data Analysis (EDA) and Insight generation with\n",
      "the help of latest Gemini engine by Google\n",
      "•EDA generation with the help of AutoViz and Image analysis with Gemini-Pro-Vision.\n",
      "•Hallucination check with Trulense-Eval.\n",
      "•UI with Streamlit.\n",
      "TradePilot Git Link , Empowering stock market price prediction with sentiment analysis from News,\n",
      "Twitter and Reddit\n",
      "•Stock price history collected from Yahoo Finance and Text data scraped from Economic Times,\n",
      "Reddit, and Twitter.\n",
      "•Sentiment analysis with Finbert (Hugging Face) and Summary with Google Gemini-Pro.\n",
      "•UI with FastAPI.\n",
      "SKILLS\n",
      "Machine Learning ( scikit ,NLTK ,spaCy ,Numpy ,Pandas ,NLP )\n",
      "Deep Learning ( Keras ,PyTorch ,TensorFlow ,RNN ,LSTM ,CNN )\n",
      "Data Visualization ( Tableau ,PowerBI )\n",
      "Database Management ( MySQL ,MongoDB )\n",
      "Programming Languages ( Python ,JavaScript ,C,C++)\n",
      "UI Development ( FastAPI ,HTML ,CSS,Streamlit ,Django ).\n",
      "EDUCATION\n",
      "Data Science Self Learning Bootcamp, Brototype Ernakulam, India\n",
      "•Covered topics including Data Science, Machine Learning, Deep Learning, Generative AI,\n",
      "NLP, Feature Engineering, RAG Modeling, Data Structures, Statistics and Probability, Web\n",
      "Development.\n",
      "•Trained in communication and personality development.\n",
      "B.Tech in Naval Architecture, CUSAT Kerala, India\n",
      "•Studied Ship Building, Stability, Hydrodynamics, Mechanics, Graphics, and Computer Science.\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Naval Architect, SEDS 07/2020 – 03/2023 — Kochi, India\n",
      "•Contributed to the development of shipbuilding projects, adhering to industry standards.\n",
      "Production Supervisor, Navgathi 07/2019 – 03/2020 — Kochi, India\n",
      "•Managed the production team for boat production.\n",
      "CERTIFICATES\n",
      "Machine Learning (Kaggle) ,Data Science (Coursera) ,HTML (Codeacademy) .\n",
      "As a Junior Machine Learning Engineer at our cutting-edge company, your role is crucial in developing the future of AI chatbots and Large Language Models (LLMs) like Llama2, with a special emphasis on integrating these innovations with WIX platforms. Your work will involve a deep dive into AWS services, Data ETL, GitHub for code collaboration, and the hands-on building of models, all while leveraging your strong quantitative and programming background. This role demands not only technical expertise but also a creative approach to problem-solving and code development. You will work in close partnership with the CTO, ensuring that your contributions leave a lasting impact on our AI-driven solutions. Additionally, your collaboration with front-end developers is key to weaving AI functionalities into WIX-based applications seamlessly, enhancing user experiences and pushing the boundaries of what our digital platforms can achieve.\n"
     ]
    }
   ],
   "source": [
    "for chunks in mergeds.get_relevant_documents(\"Machine Learning\"):\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in merged.get_relevant_documents(\"Machine Learning\"):\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Presets using FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='VISHNU PRAKASH\\nData Scientist\\nvishnucheppanam@gmail.com — +91 80 780 43398\\nlinkedin.com/in/vishnuprksh — github.com/vishnuprksh — vishnuprakash.online\\nPROFILE\\nData Science aspirant with hands-on experience in machine learning and deep learning, seeking a\\nchallenging role as a Data Scientist/Machine Learning Engineer. Eager to leverage skills in Artificial\\nIntelligence, Neural Networks, and Algorithms to contribute to the companies and personal growth.\\nPROJECTS\\nGemInsights Git Link , Automating Exploratory Data Analysis (EDA) and Insight generation with\\nthe help of latest Gemini engine by Google\\n•EDA generation with the help of AutoViz and Image analysis with Gemini-Pro-Vision.\\n•Hallucination check with Trulense-Eval.\\n•UI with Streamlit.\\nTradePilot Git Link , Empowering stock market price prediction with sentiment analysis from News,\\nTwitter and Reddit\\n•Stock price history collected from Yahoo Finance and Text data scraped from Economic Times,\\nReddit, and Twitter.\\n•Sentiment analysis with Finbert (Hugging Face) and Summary with Google Gemini-Pro.\\n•UI with FastAPI.\\nSKILLS\\nMachine Learning ( scikit ,NLTK ,spaCy ,Numpy ,Pandas ,NLP )\\nDeep Learning ( Keras ,PyTorch ,TensorFlow ,RNN ,LSTM ,CNN )\\nData Visualization ( Tableau ,PowerBI )\\nDatabase Management ( MySQL ,MongoDB )\\nProgramming Languages ( Python ,JavaScript ,C,C++)\\nUI Development ( FastAPI ,HTML ,CSS,Streamlit ,Django ).\\nEDUCATION\\nData Science Self Learning Bootcamp, Brototype Ernakulam, India\\n•Covered topics including Data Science, Machine Learning, Deep Learning, Generative AI,\\nNLP, Feature Engineering, RAG Modeling, Data Structures, Statistics and Probability, Web\\nDevelopment.\\n•Trained in communication and personality development.\\nB.Tech in Naval Architecture, CUSAT Kerala, India\\n•Studied Ship Building, Stability, Hydrodynamics, Mechanics, Graphics, and Computer Science.\\nPROFESSIONAL EXPERIENCE\\nNaval Architect, SEDS 07/2020 – 03/2023 — Kochi, India\\n•Contributed to the development of shipbuilding projects, adhering to industry standards.\\nProduction Supervisor, Navgathi 07/2019 – 03/2020 — Kochi, India\\n•Managed the production team for boat production.\\nCERTIFICATES\\nMachine Learning (Kaggle) ,Data Science (Coursera) ,HTML (Codeacademy) .' metadata={'source': 'Testing/resume/VishnuPrakash_Resume.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Loading Resume\n",
    "resume = PyPDFLoader(\"Testing/resume/VishnuPrakash_Resume.pdf\")\n",
    "resume_load = resume.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "resume_splitted = text_splitter.split_documents(resume_load)\n",
    "\n",
    "print(resume_splitted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Junior Machine Learning Engineer at our cutting-edge company, your role is crucial in developing the future of AI chatbots and Large Language Models (LLMs) like Llama2, with a special emphasis on integrating these innovations with WIX platforms. Your work will involve a deep dive into AWS services, Data ETL, GitHub for code collaboration, and the hands-on building of models, all while leveraging your strong quantitative and programming background. This role demands not only technical expertise but also a creative approach to problem-solving and code development. You will work in close partnership with the CTO, ensuring that your contributions leave a lasting impact on our AI-driven solutions. Additionally, your collaboration with front-end developers is key to weaving AI functionalities into WIX-based applications seamlessly, enhancing user experiences and pushing the boundaries of what our digital platforms can achieve.\n"
     ]
    }
   ],
   "source": [
    "path = \"Testing/resume/job_desc.txt\"\n",
    "with open(path, \"r\") as file:\n",
    "    job_desc = file.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "jd_splitted = text_splitter.split_text(job_desc)\n",
    "\n",
    "print(jd_splitted[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectore storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FAISS.__init__() got an unexpected keyword argument 'persist_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# vectorDB for resume\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chroma_resume \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_splitted\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_storage/resume_store\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# l2 is the default\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# vectorDB for JD\u001b[39;00m\n\u001b[1;32m      7\u001b[0m chroma_jd \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m      8\u001b[0m     jd_splitted,embeddings,\n\u001b[1;32m      9\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_storage/jd_store\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_core/vectorstores.py:528\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    527\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:966\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \n\u001b[1;32m    949\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    965\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m--> 966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:926\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[1;32m    925\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m--> 926\u001b[0m vecstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_to_docstore_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_L2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_L2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m vecstore\u001b[38;5;241m.\u001b[39m__add(texts, embeddings, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vecstore\n",
      "\u001b[0;31mTypeError\u001b[0m: FAISS.__init__() got an unexpected keyword argument 'persist_directory'"
     ]
    }
   ],
   "source": [
    "# vectorDB for resume\n",
    "chroma_resume = FAISS.from_documents(\n",
    "        resume_splitted,embeddings,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},persist_directory=\"vector_storage/resume_store\" # l2 is the default\n",
    "    )\n",
    "# vectorDB for JD\n",
    "chroma_jd = FAISS.from_texts(\n",
    "    jd_splitted,embeddings,collection_metadata = {\"hnsw:space\": \"cosine\"},\n",
    "    persist_directory=\"vector_storage/jd_store\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# def embeddings(text):\n",
    "# \ttext_splitter = SpacyTextSplitter()\n",
    "# \ttext_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\")\n",
    "# \ttexts = text_splitter.split_text(text)\n",
    "# \tembeddings = OpenAIEmbeddings()\n",
    "# \tdocsearch = FAISS.from_texts(texts, embeddings)\n",
    "# \tretriever = docsearch.similarity_search(text)\n",
    "# \treturn retriever\n",
    "# def save_vector(resume):\n",
    "#     \"\"\"create embeddings for Resume \"\"\"\n",
    "#     with open(resume,'rb') as f:\n",
    "#         pdf_reader = PdfReader(resume)\n",
    "#         text = \"\"\n",
    "#         for page in pdf_reader.pages:\n",
    "#             text += page.extract_text()\n",
    "#     # Split the document into chunks\n",
    "#     text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#         encoding=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    "#         )\n",
    "#     texts = text_splitter.split_text(text)\n",
    "\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#     docsearch = FAISS.from_texts(texts, embeddings)\n",
    "#     retriever = docsearch.as_retriever(search_type='similarity search')\n",
    "#     return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loader = PyPDFLoader(\"Testing/resume/VishnuPrakash_Resume.pdf\")\n",
    "# # pages = loader.load_and_split()\n",
    "# val = save_vector(loader)\n",
    "# print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
