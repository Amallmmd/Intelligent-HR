{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "def embeddings(text):\n",
    "\ttext_splitter = SpacyTextSplitter()\n",
    "\ttext_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\")\n",
    "\ttexts = text_splitter.split_text(text)\n",
    "\tembeddings = OpenAIEmbeddings()\n",
    "\tdocsearch = FAISS.from_texts(texts, embeddings)\n",
    "\tretriever = docsearch.similarity_search(text)\n",
    "\treturn retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "load_dotenv()\n",
    "import os\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "@dataclass\n",
    "class Message:\n",
    "\torigin: Literal[\"human\", \"ai\"]\n",
    "\tmessage: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA, ConversationChain\n",
    "from prompts.prompts import Template\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "# from pypdf import PdfReader\n",
    "# from prompts.prompt_selector import prompt_sector\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "def embeddings(text):\n",
    "    text_splitter = NLTKTextSplitter()\n",
    "    texts = text_splitter.split_text(text)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "    docsearch = FAISS.from_documents(texts, embeddings)     \n",
    "    return docsearch\n",
    "\n",
    "def resume_reader(resume):\n",
    "    pdf_reader = PdfReader(resume)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1634487d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enthusiastic and self-motivated Computer Science graduate seeking a entry-level position within a large\n",
      "organisation as a Data Scientist or Machine Learning Developer. Possess excellent communication and\n",
      "collaboration skills, eager to learn and grow within a supportive team environment. Ready to leverage my\n",
      "problem-solving and data visualisation skills to contribute to data-driven decision-making within a dynamic\n",
      "company.\n",
      "Python Proficiency\n",
      "SQL Database\n",
      "Machine Learning Models\n",
      "Open CVNatural Language Processing\n",
      "Numpy and Pandas\n",
      "Tensorflow\n",
      "GitPytorch\n",
      "Deep Learning Models\n",
      "Developing Generative Models\n",
      "Gemini Hackathon\n",
      "Bachelor of Computer Application National Service Scheme ( NSS )\n",
      "High SchoolHackathonsGemInsights\n",
      "Majors: Maths and Computer Science MES College \n",
      "Kerala State Public Board\n",
      "94.3  %MES College Developed a chatbot that automates the Exploratory Data Analysis cycle and generates insights regarding\n",
      "the visualisation from the datasets. This enables user to understand the overall summary of the particular\n",
      "dataset\n",
      "Technolgy Used :  Streamlit, Gemini LLM, Langchain, AutoViz, ML PipellineDecember 2023 PROFILE INTRODUCTION+917025110647 \n",
      "amalmuhammed6677@gmail.com \n",
      "Kochi, Kerala\n",
      "https://www.linkedin.com/in/amal-muhammed-48950724a/AMAL MUHAMMED\n",
      "PROJECT EXPERIENCE\n",
      "EDUCATION & CERTIFICATIONS EXTRACURRICULAR ACTIVITIESHARD SKILLS\n",
      "Mahatma Gandhi University\n",
      "7.3 CGPACommunication Skills\n",
      "Collaborative Work Experience\n",
      "Numerical AnalysisInterpersonal Skills\n",
      "Self Learning\n",
      "Critical ThinkingConvey Complex Concepts\n",
      "Business Understanding\n",
      "Problem SolvingSOFT SKILLS\n",
      "Github : https://github.com/Amallmmd\n"
     ]
    }
   ],
   "source": [
    "path = \"Testing/resume/Resume Jan 2024.pdf\"\n",
    "# with open(path,'rb') as fi:\n",
    "#     resume = fi.read()\n",
    "#     # content = resume.extract_text()\n",
    "# # print(resume_reader(resume))\n",
    "\n",
    "\n",
    "\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# loader = PyPDFLoader(path)\n",
    "# pages = loader.load_and_split()\n",
    "# pages\n",
    "\n",
    "\n",
    "resume_data = resume_reader(path)\n",
    "# print(resume_data)\n",
    "# print(embeddings(resume_data))\n",
    "\n",
    "vector = embeddings(resume_data)\n",
    "\n",
    "print(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'docsearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Test the initialize_session_state function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjd\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob description\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[43minitialize_session_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocsearch\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResume data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[48], line 14\u001b[0m, in \u001b[0;36minitialize_session_state\u001b[0;34m(template, position)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" initialize session states \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m st\u001b[38;5;241m.\u001b[39msession_state:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocsearch\u001b[49m \u001b[38;5;241m=\u001b[39m embeddings(st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjd\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mdocsearch \u001b[38;5;241m=\u001b[39m embeddings(resume_reader(st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'docsearch'"
     ]
    }
   ],
   "source": [
    "class SessionStateMock:\n",
    "    def __init__(self):\n",
    "        self.session_state = {}\n",
    "\n",
    "# Instantiate a mock session state\n",
    "st = SessionStateMock()\n",
    "\n",
    "\n",
    "\n",
    "# Define the initialize_session_state function\n",
    "def initialize_session_state(template=None, position=None):\n",
    "    \"\"\" initialize session states \"\"\"\n",
    "    if 'jd' in st.session_state:\n",
    "        st.session_state.docsearch = embeddings(st.session_state['jd'])\n",
    "    else:\n",
    "        st.session_state.docsearch = embeddings(resume_reader(st.session_state['resume']))\n",
    "\n",
    "# Test the initialize_session_state function\n",
    "st.session_state['jd'] = \"Job description\"\n",
    "initialize_session_state()\n",
    "print(st.session_state['docsearch'])\n",
    "\n",
    "st.session_state['resume'] = \"Resume data\"\n",
    "initialize_session_state()\n",
    "print(st.session_state['docsearch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# from langchain.globals import set_llm_cache\n",
    "\n",
    "# from langchain.cache import InMemoryCache\n",
    "# set_llm_cache(InMemoryCache())\n",
    "# def load_llm():\n",
    "#     # Load the locally downloaded model here\n",
    "#     llm = OpenAI(\n",
    "#         model_name=\"gpt-3.5-turbo-instruct\",\n",
    "#         max_new_tokens=50,\n",
    "#         temperature=0.5\n",
    "#     )\n",
    "#     return llm\n",
    "# # llm = load_llm()\n",
    "# # prompt = templates()\n",
    "# # Retrieval QA Chain\n",
    "# def retrieval_qa_chain(llm, prompt, db):\n",
    "# #     qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "# #                                            chain_type='stuff',\n",
    "# #                                            retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "# #                                            return_source_documents=True,\n",
    "# #                                            chain_type_kwargs={'prompt': prompt}\n",
    "# #                                            )\n",
    "# #     template = (\n",
    "# #     \"Combine the chat history and follow up question into \"\n",
    "# #     \"a standalone question. Chat History: {chat_history}\"\n",
    "# #     \"Follow up question: {question}\"\n",
    "# # )\n",
    "# #     prompt = PromptTemplate.from_template(template)\n",
    "#     # retriever=db.as_retriever()\n",
    "#     question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#     qa_chain = ConversationalRetrievalChain(\n",
    "#         # retriever=retriever,\n",
    "#         question_generator=question_generator_chain,\n",
    "#     )\n",
    "#     return qa_chain\n",
    "\n",
    "# # QA Model Function\n",
    "# def qa_bot(text):\n",
    "#     db = embeddings(text)\n",
    "#     llm = load_llm()\n",
    "#     qa_prompt = templates()\n",
    "#     qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "\n",
    "#     return qa\n",
    "\n",
    "# # Output function\n",
    "# def final_result(query):\n",
    "#     qa_result = qa_bot(query)\n",
    "#     response = qa_result({'query': query})\n",
    "#     return response\n",
    "\n",
    "# # Behavioral_Prompt = PromptTemplate(input_variables=[\"context\", \"question\"],\n",
    "# #                                           template=templates.behavioral_template)\n",
    "# job = \"Obvious Technology Inc. is a cognitive enterprise platform that simplifies AI for businesses. We use deep technology, including computer vision, natural language processing, and machine learning, to build a cognitive platform powered by our proprietary AiBlock™. Our goal is to provide affordable AI solutions to our clients in industries such as healthcare, banking, insurance, financial services, manufacturing, and retail. With 11 patents, 24 proprietary AiBlocks™, and over 60 business use cases developed, we are dedicated to making AI accessible to all\"\n",
    "# print(final_result(job))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
