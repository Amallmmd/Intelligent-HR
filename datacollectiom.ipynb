{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def templates():\n",
    "\t\"\"\" store all prompts templates \"\"\"\n",
    "\tbehavioral_template = \"\"\" I want you to act as an interviewer. Remember, you are the interviewer not the candidate.   \n",
    "            Let's think step by step.\n",
    "            \n",
    "            Based on the keywords, \n",
    "            Create a guideline with the following topics for a behavioral interview to test the soft skills of the candidate. \n",
    "            \n",
    "            Do not ask the same question.\n",
    "            Do not repeat the question. \n",
    "            \n",
    "            Keywords: \n",
    "            {context}\n",
    "       \n",
    "            Question: {question}\n",
    "            Answer:\"\"\"\n",
    "\tconversation_template = \"\"\"I want you to act as an interviewer strictly following the guideline in the current conversation.\n",
    "                            Candidate has no idea what the guideline is.\n",
    "                            Ask me questions and wait for my answers. Do not write explanations.\n",
    "                            Ask each question like a real person, only one question at a time.\n",
    "                            Do not ask the same question.\n",
    "                            Do not repeat the question.\n",
    "                            Do ask follow-up questions if necessary. \n",
    "                            Your name is GPTInterviewer.\n",
    "                            I want you to only reply as an interviewer.\n",
    "                            Do not write all the conversation at once.\n",
    "                            If there is an error, point it out.\n",
    "\n",
    "                            Current Conversation:\n",
    "                            {history}\n",
    "\n",
    "                            Candidate: {input}\n",
    "                            AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "def embeddings(text):\n",
    "\ttext_splitter = SpacyTextSplitter()\n",
    "\ttext_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\")\n",
    "\ttexts = text_splitter.split_text(text)\n",
    "\tembeddings = OpenAIEmbeddings()\n",
    "\tdocsearch = FAISS.from_texts(texts, embeddings)\n",
    "\tretriever = docsearch.similarity_search(text)\n",
    "\treturn retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "load_dotenv()\n",
    "import os\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "@dataclass\n",
    "class Message:\n",
    "\torigin: Literal[\"human\", \"ai\"]\n",
    "\tmessage: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amallmuhammed/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/Users/amallmuhammed/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! max_new_tokens is not default parameter.\n",
      "                max_new_tokens was transferred to model_kwargs.\n",
      "                Please confirm that max_new_tokens is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nprompt\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Behavioral_Prompt = PromptTemplate(input_variables=[\"context\", \"question\"],\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#                                           template=templates.behavioral_template)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObvious Technology Inc. is a cognitive enterprise platform that simplifies AI for businesses. We use deep technology, including computer vision, natural language processing, and machine learning, to build a cognitive platform powered by our proprietary AiBlock™. Our goal is to provide affordable AI solutions to our clients in industries such as healthcare, banking, insurance, financial services, manufacturing, and retail. With 11 patents, 24 proprietary AiBlocks™, and over 60 business use cases developed, we are dedicated to making AI accessible to all\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[34], line 49\u001b[0m, in \u001b[0;36mfinal_result\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinal_result\u001b[39m(query):\n\u001b[0;32m---> 49\u001b[0m     qa_result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     response \u001b[38;5;241m=\u001b[39m qa_result({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: query})\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[34], line 43\u001b[0m, in \u001b[0;36mqa_bot\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     41\u001b[0m llm \u001b[38;5;241m=\u001b[39m load_llm()\n\u001b[1;32m     42\u001b[0m qa_prompt \u001b[38;5;241m=\u001b[39m templates()\n\u001b[0;32m---> 43\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_qa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qa\n",
      "Cell \u001b[0;32mIn[34], line 31\u001b[0m, in \u001b[0;36mretrieval_qa_chain\u001b[0;34m(llm, prompt, db)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieval_qa_chain\u001b[39m(llm, prompt, db):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     qa_chain = RetrievalQA.from_chain_type(llm=llm,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#                                            chain_type='stuff',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     prompt = PromptTemplate.from_template(template)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# retriever=db.as_retriever()\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     question_generator_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     qa_chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# retriever=retriever,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         question_generator\u001b[38;5;241m=\u001b[39mquestion_generator_chain,\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qa_chain\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/Bro Project/Intelligent-HR/project/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LLMChain\nprompt\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "def load_llm():\n",
    "    # Load the locally downloaded model here\n",
    "    llm = OpenAI(\n",
    "        model_name=\"gpt-3.5-turbo-instruct\",\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return llm\n",
    "# llm = load_llm()\n",
    "# prompt = templates()\n",
    "# Retrieval QA Chain\n",
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "#     qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "#                                            chain_type='stuff',\n",
    "#                                            retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "#                                            return_source_documents=True,\n",
    "#                                            chain_type_kwargs={'prompt': prompt}\n",
    "#                                            )\n",
    "#     template = (\n",
    "#     \"Combine the chat history and follow up question into \"\n",
    "#     \"a standalone question. Chat History: {chat_history}\"\n",
    "#     \"Follow up question: {question}\"\n",
    "# )\n",
    "#     prompt = PromptTemplate.from_template(template)\n",
    "    # retriever=db.as_retriever()\n",
    "    question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    qa_chain = ConversationalRetrievalChain(\n",
    "        # retriever=retriever,\n",
    "        question_generator=question_generator_chain,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "# QA Model Function\n",
    "def qa_bot(text):\n",
    "    db = embeddings(text)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = templates()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "\n",
    "    return qa\n",
    "\n",
    "# Output function\n",
    "def final_result(query):\n",
    "    qa_result = qa_bot(query)\n",
    "    response = qa_result({'query': query})\n",
    "    return response\n",
    "\n",
    "# Behavioral_Prompt = PromptTemplate(input_variables=[\"context\", \"question\"],\n",
    "#                                           template=templates.behavioral_template)\n",
    "job = \"Obvious Technology Inc. is a cognitive enterprise platform that simplifies AI for businesses. We use deep technology, including computer vision, natural language processing, and machine learning, to build a cognitive platform powered by our proprietary AiBlock™. Our goal is to provide affordable AI solutions to our clients in industries such as healthcare, banking, insurance, financial services, manufacturing, and retail. With 11 patents, 24 proprietary AiBlocks™, and over 60 business use cases developed, we are dedicated to making AI accessible to all\"\n",
    "print(final_result(job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
